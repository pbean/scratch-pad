name: Performance Testing and Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run performance tests daily at 4 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Platforms to test (JSON array)'
        required: false
        default: '["ubuntu-latest", "windows-latest", "macos-latest"]'
        type: string
      performance-monitoring:
        description: 'Enable detailed performance monitoring'
        required: false
        default: true
        type: boolean
      test-coverage:
        description: 'Generate test coverage reports'
        required: false
        default: false
        type: boolean

env:
  CARGO_TERM_COLOR: always

jobs:
  performance-test:
    name: Performance Test Suite
    uses: ./.github/workflows/reusable-test.yml
    with:
      platforms: ${{ github.event.inputs.platforms || '["ubuntu-latest", "windows-latest", "macos-latest"]' }}
      performance-monitoring: ${{ github.event.inputs.performance-monitoring == 'true' || github.event_name == 'schedule' }}
      test-coverage: ${{ github.event.inputs.test-coverage == 'true' }}
      fail-fast: false

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: performance-test
    if: always() && (github.event.inputs.performance-monitoring == 'true' || github.event_name == 'schedule')
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Setup Node.js and PNPM
        uses: ./.github/actions/setup-node-pnpm

      - name: Download performance metrics
        uses: actions/download-artifact@v4
        with:
          name: aggregated-metrics
          path: ./performance-data

      - name: Install analysis tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc
          pnpm add -g @types/node typescript ts-node

      - name: Analyze performance trends
        shell: bash
        run: |
          # Create performance analysis script
          cat > analyze-performance.ts << 'EOF'
          import { readFileSync, writeFileSync } from 'fs';
          
          interface PerformanceMetric {
            test_type: string;
            start_time: string;
            end_time: string;
            result: string;
            exit_code: number;
            timeout_seconds: number;
            platform?: string;
          }
          
          interface TestResult {
            platform: string;
            frontend_tests: string;
            backend_tests: string;
            integration_tests: string;
            build_success: string;
            timestamp: string;
          }
          
          interface MetricsData {
            performance_metrics: PerformanceMetric[];
            test_results: TestResult[];
            aggregation_timestamp: string;
          }
          
          function calculateDuration(start: string, end: string): number {
            return new Date(end).getTime() - new Date(start).getTime();
          }
          
          function analyzeMetrics(data: MetricsData) {
            const analysis = {
              summary: {
                total_platforms: data.test_results.length,
                successful_platforms: data.test_results.filter(r => 
                  r.frontend_tests === 'success' && 
                  r.backend_tests === 'success' && 
                  r.integration_tests === 'success'
                ).length,
                total_test_duration_ms: 0,
                average_test_duration_ms: 0
              },
              by_platform: {} as Record<string, any>,
              performance_insights: [] as string[]
            };
          
            // Analyze by platform
            for (const result of data.test_results) {
              const platformMetrics = data.performance_metrics.filter(m => m.platform === result.platform);
              const totalDuration = platformMetrics.reduce((sum, metric) => 
                sum + calculateDuration(metric.start_time, metric.end_time), 0
              );
              
              analysis.by_platform[result.platform] = {
                success_rate: [result.frontend_tests, result.backend_tests, result.integration_tests]
                  .filter(r => r === 'success').length / 3,
                total_duration_ms: totalDuration,
                test_breakdown: platformMetrics.map(m => ({
                  type: m.test_type,
                  duration_ms: calculateDuration(m.start_time, m.end_time),
                  success: m.result === 'success'
                }))
              };
              
              analysis.summary.total_test_duration_ms += totalDuration;
            }
          
            analysis.summary.average_test_duration_ms = 
              analysis.summary.total_test_duration_ms / data.test_results.length;
          
            // Generate insights
            const slowestPlatform = Object.entries(analysis.by_platform)
              .sort(([,a], [,b]) => (b as any).total_duration_ms - (a as any).total_duration_ms)[0];
            
            if (slowestPlatform) {
              analysis.performance_insights.push(
                `Slowest platform: ${slowestPlatform[0]} (${Math.round((slowestPlatform[1] as any).total_duration_ms / 1000)}s)`
              );
            }
          
            const failedPlatforms = data.test_results.filter(r => 
              r.frontend_tests !== 'success' || r.backend_tests !== 'success' || r.integration_tests !== 'success'
            );
            
            if (failedPlatforms.length > 0) {
              analysis.performance_insights.push(
                `Failed platforms: ${failedPlatforms.map(p => p.platform).join(', ')}`
              );
            }
          
            return analysis;
          }
          
          try {
            const metricsFile = './performance-data/final-metrics.json';
            const data: MetricsData = JSON.parse(readFileSync(metricsFile, 'utf8'));
            const analysis = analyzeMetrics(data);
            
            writeFileSync('./performance-analysis.json', JSON.stringify(analysis, null, 2));
            console.log('Performance Analysis Complete');
            console.log(JSON.stringify(analysis, null, 2));
          } catch (error) {
            console.error('Error analyzing performance metrics:', error);
            process.exit(1);
          }
          EOF
          
          # Run analysis
          npx ts-node analyze-performance.ts

      - name: Generate performance report
        shell: bash
        run: |
          if [ -f "./performance-analysis.json" ]; then
            echo "## 📊 Performance Test Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Extract key metrics
            success_rate=$(jq -r '.summary.successful_platforms / .summary.total_platforms * 100' performance-analysis.json)
            avg_duration=$(jq -r '.summary.average_test_duration_ms / 1000' performance-analysis.json)
            
            echo "### Summary" >> $GITHUB_STEP_SUMMARY
            echo "- **Success Rate**: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
            echo "- **Average Test Duration**: ${avg_duration}s" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Platforms**: $(jq -r '.summary.total_platforms' performance-analysis.json)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "### Performance Insights" >> $GITHUB_STEP_SUMMARY
            jq -r '.performance_insights[] | "- " + .' performance-analysis.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "### Platform Breakdown" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            jq '.by_platform' performance-analysis.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Performance thresholds
            if (( $(echo "$avg_duration > 120" | bc -l) )); then
              echo "⚠️ **Warning**: Average test duration exceeds 2 minutes" >> $GITHUB_STEP_SUMMARY
            fi
            
            if (( $(echo "$success_rate < 95" | bc -l) )); then
              echo "❌ **Alert**: Success rate below 95%" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "❌ Performance analysis failed - no metrics file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-${{ github.run_number }}
          path: |
            ./performance-analysis.json
            ./performance-data/
          retention-days: 30

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('./performance-analysis.json')) {
              const analysis = JSON.parse(fs.readFileSync('./performance-analysis.json', 'utf8'));
              const successRate = (analysis.summary.successful_platforms / analysis.summary.total_platforms * 100).toFixed(1);
              const avgDuration = (analysis.summary.average_test_duration_ms / 1000).toFixed(1);
              
              const comment = `## 📊 Performance Test Results
              
              **Summary:**
              - Success Rate: ${successRate}%
              - Average Duration: ${avgDuration}s
              - Platforms Tested: ${analysis.summary.total_platforms}
              
              **Insights:**
              ${analysis.performance_insights.map(insight => `- ${insight}`).join('\n')}
              
              <details>
              <summary>Platform Breakdown</summary>
              
              \`\`\`json
              ${JSON.stringify(analysis.by_platform, null, 2)}
              \`\`\`
              </details>
              
              *Generated by Performance Testing Workflow*`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }